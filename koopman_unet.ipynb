{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T18:36:04.036011Z",
     "start_time": "2025-11-17T18:36:04.032374Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from diffusers import DDPMScheduler, UNet2DModel, DDPMPipeline\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T18:34:53.984615Z",
     "start_time": "2025-11-17T18:34:53.982100Z"
    }
   },
   "cell_type": "code",
   "source": "KOOPMAN = True",
   "id": "6bab6db26528cdb5",
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "177893403fba2525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T18:34:54.242749Z",
     "start_time": "2025-11-17T18:34:54.238618Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 32\n",
    "    train_batch_size = 64\n",
    "    eval_batch_size = 64\n",
    "    num_epochs = 2\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 1\n",
    "    save_model_epochs = 2\n",
    "    output_dir = \"ddpm_mnist_koopman\" if KOOPMAN else \"ddpm_mnist\"\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()\n",
    "os.makedirs(config.output_dir, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "3343b356cb4a138b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T18:34:55.782475Z",
     "start_time": "2025-11-17T18:34:55.778865Z"
    }
   },
   "source": [
    "def get_mnist_dataloader(image_size, batch_size):\n",
    "    \"\"\"Loads the MNIST dataset.\"\"\"\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),  # (pixel - 0.5) / 0.5\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = torchvision.datasets.MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=preprocess\n",
    "    )\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "id": "c6fed7ae-dc14-4ea4-8928-bac6044d4b71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T18:34:56.469820Z",
     "start_time": "2025-11-17T18:34:56.458809Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.models.embeddings import Timesteps, TimestepEmbedding\n",
    "from diffusers.models.unets.unet_2d_blocks import (\n",
    "    UNetMidBlock2D,\n",
    "    get_down_block,\n",
    "    get_up_block,\n",
    ")\n",
    "\n",
    "class KoopmanUNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        image_size = config.sample_size\n",
    "        in_channels = config.in_channels\n",
    "        out_channels = config.out_channels\n",
    "        block_out_channels = config.block_out_channels\n",
    "        layers_per_block = config.layers_per_block\n",
    "        down_block_types = config.down_block_types\n",
    "        up_block_types = config.up_block_types\n",
    "\n",
    "        time_embed_dim = block_out_channels[0] * 4\n",
    "\n",
    "        self.time_proj = Timesteps(block_out_channels[0], flip_sin_to_cos=True, downscale_freq_shift=0)\n",
    "        self.time_embedding = TimestepEmbedding(\n",
    "            in_channels=block_out_channels[0],\n",
    "            time_embed_dim=time_embed_dim,\n",
    "        )\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, padding=1)\n",
    "\n",
    "        self.down_blocks = nn.ModuleList([])\n",
    "        output_channel = block_out_channels[0]\n",
    "        for i, down_block_type in enumerate(down_block_types):\n",
    "            input_channel = output_channel\n",
    "            output_channel = block_out_channels[i]\n",
    "            is_final_block = i == len(down_block_types) - 1\n",
    "\n",
    "            down_block = get_down_block(\n",
    "                down_block_type,\n",
    "                num_layers=layers_per_block,\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channel,\n",
    "                temb_channels=time_embed_dim,\n",
    "                add_downsample=not is_final_block,\n",
    "                resnet_eps=1e-5,\n",
    "                resnet_act_fn=\"silu\",\n",
    "                resnet_groups=32,\n",
    "                attention_head_dim=8,\n",
    "                downsample_padding=1,  # <-- FIX 1: Added padding\n",
    "            )\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        self.mid_block = UNetMidBlock2D(\n",
    "            in_channels=block_out_channels[-1],\n",
    "            temb_channels=time_embed_dim,\n",
    "            resnet_eps=1e-5,\n",
    "            resnet_act_fn=\"silu\",\n",
    "            output_scale_factor=1,\n",
    "            attention_head_dim=8, # Use this arg for MidBlock\n",
    "            resnet_groups=32,\n",
    "        )\n",
    "\n",
    "        self.bottleneck_c = block_out_channels[-1]\n",
    "        downsample_factor = 2 ** (len(down_block_types) - 1)\n",
    "        self.bottleneck_h = image_size // downsample_factor\n",
    "        self.bottleneck_w = image_size // downsample_factor\n",
    "        self.bottleneck_features = self.bottleneck_c * self.bottleneck_h * self.bottleneck_w\n",
    "\n",
    "        self.koopman_operator = nn.Linear(self.bottleneck_features, self.bottleneck_features)\n",
    "\n",
    "        print(f\"Koopman Bottleneck Initialized:\")\n",
    "        print(f\"  Shape: ({self.bottleneck_c}, {self.bottleneck_h}, {self.bottleneck_w})\")\n",
    "        print(f\"  Total Features: {self.bottleneck_features}\")\n",
    "\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "        reversed_block_out_channels = list(reversed(block_out_channels))\n",
    "        output_channel = reversed_block_out_channels[0]\n",
    "        for i, up_block_type in enumerate(up_block_types):\n",
    "            prev_output_channel = output_channel\n",
    "            output_channel = reversed_block_out_channels[i]\n",
    "            input_channel = reversed_block_out_channels[min(i + 1, len(block_out_channels) - 1)]\n",
    "\n",
    "            is_final_block = i == len(up_block_types) - 1\n",
    "\n",
    "            up_block = get_up_block(\n",
    "                up_block_type,\n",
    "                num_layers=layers_per_block + 1,\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channel,\n",
    "                prev_output_channel=prev_output_channel,\n",
    "                temb_channels=time_embed_dim,\n",
    "                add_upsample=not is_final_block,\n",
    "                resnet_eps=1e-5,\n",
    "                resnet_act_fn=\"silu\",\n",
    "                resnet_groups=32,\n",
    "                attention_head_dim=8,\n",
    "            )\n",
    "            self.up_blocks.append(up_block)\n",
    "            prev_output_channel = output_channel\n",
    "\n",
    "        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=32, eps=1e-5)\n",
    "        self.conv_act = nn.SiLU()\n",
    "        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, t, return_dict=False):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # 1. Time Embedding\n",
    "        t_emb = self.time_proj(t)\n",
    "        t_emb = self.time_embedding(t_emb)\n",
    "\n",
    "        # 2. Input\n",
    "        x = self.conv_in(x)\n",
    "\n",
    "        # --- 3. ENCODER (Corrected Skip Logic) ---\n",
    "        # The output of conv_in is the first skip connection\n",
    "        skip_connections = (x,)\n",
    "\n",
    "        for block in self.down_blocks:\n",
    "            x, skips = block(x, t_emb)\n",
    "            # Add the (2) skips from this block to our giant tuple\n",
    "            skip_connections += skips\n",
    "\n",
    "        # 4. MID BLOCK\n",
    "        x = self.mid_block(x, t_emb)\n",
    "\n",
    "        # 5. KOOPMAN BOTTLENECK\n",
    "        x = x.reshape(batch_size, -1) # Use .reshape()\n",
    "        x = self.koopman_operator(x)\n",
    "        x = x.reshape(batch_size, self.bottleneck_c, self.bottleneck_h, self.bottleneck_w)\n",
    "\n",
    "        # --- 6. DECODER (Corrected Skip Logic) ---\n",
    "        for block in self.up_blocks:\n",
    "            # Get the number of resnets in *this* block (e.g., 3)\n",
    "            num_skips = len(block.resnets)\n",
    "\n",
    "            # Get the last 'num_skips' from our big tuple\n",
    "            skips_for_this_block = skip_connections[-num_skips:]\n",
    "\n",
    "            # Shorten the skip_connections tuple for the next loop\n",
    "            skip_connections = skip_connections[:-num_skips]\n",
    "\n",
    "            # Pass the correct skips\n",
    "            x = block(x, skips_for_this_block, temb=t_emb)\n",
    "\n",
    "        # 7. OUTPUT\n",
    "        x = self.conv_norm_out(x)\n",
    "        x = self.conv_act(x)\n",
    "        x = self.conv_out(x)\n",
    "\n",
    "        return (x,)"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "id": "f5bdd8c4be866dfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T18:34:59.583741Z",
     "start_time": "2025-11-17T18:34:59.580087Z"
    }
   },
   "source": [
    "def setup_model_and_scheduler(image_size, koopman=False):\n",
    "    \"\"\"Initializes the U-Net model and noise scheduler.\"\"\"\n",
    "    baseline_model = UNet2DModel(\n",
    "        sample_size=image_size,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        layers_per_block=2,\n",
    "        block_out_channels=(32, 64, 128, 128),\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if koopman:\n",
    "        model = KoopmanUNet(baseline_model.config)\n",
    "    else:\n",
    "        model = baseline_model\n",
    "\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "    return model, noise_scheduler"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "id": "f4a23169ac6afcad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T18:35:01.426920Z",
     "start_time": "2025-11-17T18:35:01.421724Z"
    }
   },
   "source": [
    "def tensor_to_pil(image_tensor):\n",
    "    \"\"\"Converts a tensor image to a PIL Image.\"\"\"\n",
    "    image = (image_tensor / 2 + 0.5).clamp(0, 1) # Denormalize from [-1, 1] to [0, 1]\n",
    "    image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(img.squeeze(), 'L') for img in images]\n",
    "    return pil_images\n",
    "\n",
    "def generate_and_save_images(model, scheduler, epoch, config):\n",
    "    \"\"\"Generates images and saves them to a file.\"\"\"\n",
    "    print(f\"Generating images for epoch {epoch+1}...\")\n",
    "    model.eval()\n",
    "    pipeline = DDPMPipeline(unet=model, scheduler=scheduler)\n",
    "\n",
    "    # generate a batch of images\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    image_grid = torchvision.utils.make_grid(\n",
    "        [transforms.ToTensor()(img) for img in images], nrow=8\n",
    "    )\n",
    "\n",
    "    pil_grid = transforms.ToPILImage()(image_grid)\n",
    "    save_path = os.path.join(config.output_dir, f\"epoch_{epoch+1:04d}.png\")\n",
    "    pil_grid.save(save_path)\n",
    "    print(f\"Saved image grid to {save_path}\")\n",
    "\n",
    "    display(pil_grid)"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "id": "b16db34a469e1f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T18:36:20.514264Z",
     "start_time": "2025-11-17T18:36:20.508928Z"
    }
   },
   "source": [
    "def train_loop(config, model, noise_scheduler, optimizer, lr_scheduler, train_dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"--- Starting Training on {device} ---\")\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()  \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for step, (images, _) in enumerate(tqdm(train_dataloader)):\n",
    "            clean_images = images.to(device)\n",
    "            batch_size = clean_images.shape[0]\n",
    "\n",
    "            # sample random noise and timesteps\n",
    "            noise = torch.randn_like(clean_images)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "            # add noise to the clean images (the \"forward process\")\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            # get the model's prediction for the noise\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # 8. Update progress bar\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Average Loss: {epoch_loss / len(train_dataloader)}\")\n",
    "\n",
    "        # --- 5. Save Images and Model Checkpoint ---\n",
    "        if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            generate_and_save_images(model, noise_scheduler, epoch, config)\n",
    "\n",
    "        if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            model_path = os.path.join(config.output_dir, \"unet_model\")\n",
    "            model.save_pretrained(model_path)\n",
    "            print(f\"Saved model to {model_path}\")"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "id": "809b55962025d7cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T18:37:48.447502Z",
     "start_time": "2025-11-17T18:36:20.971533Z"
    }
   },
   "source": [
    "train_dataloader = get_mnist_dataloader(config.image_size, config.train_batch_size)\n",
    "\n",
    "model, noise_scheduler = setup_model_and_scheduler(config.image_size, koopman=KOOPMAN)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")\n",
    "\n",
    "train_loop(config, model, noise_scheduler, optimizer, lr_scheduler, train_dataloader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koopman Bottleneck Initialized:\n",
      "  Shape: (128, 4, 4)\n",
      "  Total Features: 2048\n",
      "--- Starting Training on cpu ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 61/938 [01:27<20:54,  1.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[80]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      5\u001B[39m optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n\u001B[32m      6\u001B[39m lr_scheduler = get_cosine_schedule_with_warmup(\n\u001B[32m      7\u001B[39m     optimizer=optimizer,\n\u001B[32m      8\u001B[39m     num_warmup_steps=config.lr_warmup_steps,\n\u001B[32m      9\u001B[39m     num_training_steps=(\u001B[38;5;28mlen\u001B[39m(train_dataloader) * config.num_epochs),\n\u001B[32m     10\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m \u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnoise_scheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_scheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[79]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mtrain_loop\u001B[39m\u001B[34m(config, model, noise_scheduler, optimizer, lr_scheduler, train_dataloader)\u001B[39m\n\u001B[32m     20\u001B[39m noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# get the model's prediction for the noise\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m noise_pred = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnoisy_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimesteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n\u001B[32m     25\u001B[39m loss = F.mse_loss(noise_pred, noise)\n\u001B[32m     27\u001B[39m optimizer.zero_grad()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[73]\u001B[39m\u001B[32m, line 121\u001B[39m, in \u001B[36mKoopmanUNet.forward\u001B[39m\u001B[34m(self, x, t, return_dict)\u001B[39m\n\u001B[32m    118\u001B[39m skip_connections = (x,)\n\u001B[32m    120\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.down_blocks:\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m     x, skips = \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_emb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    122\u001B[39m     \u001B[38;5;66;03m# Add the (2) skips from this block to our giant tuple\u001B[39;00m\n\u001B[32m    123\u001B[39m     skip_connections += skips\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/diffusers/models/unets/unet_2d_blocks.py:1359\u001B[39m, in \u001B[36mDownBlock2D.forward\u001B[39m\u001B[34m(self, hidden_states, temb, *args, **kwargs)\u001B[39m\n\u001B[32m   1357\u001B[39m         hidden_states = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(resnet, hidden_states, temb)\n\u001B[32m   1358\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1359\u001B[39m         hidden_states = \u001B[43mresnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1361\u001B[39m     output_states = output_states + (hidden_states,)\n\u001B[32m   1363\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.downsamplers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/diffusers/models/resnet.py:341\u001B[39m, in \u001B[36mResnetBlock2D.forward\u001B[39m\u001B[34m(self, input_tensor, temb, *args, **kwargs)\u001B[39m\n\u001B[32m    338\u001B[39m     input_tensor = \u001B[38;5;28mself\u001B[39m.downsample(input_tensor)\n\u001B[32m    339\u001B[39m     hidden_states = \u001B[38;5;28mself\u001B[39m.downsample(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m341\u001B[39m hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    343\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.time_emb_proj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    344\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.skip_time_act:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    547\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m548\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/ondrej/canada/IFT-4031/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    531\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    532\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    533\u001B[39m         F.pad(\n\u001B[32m    534\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    541\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    542\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m543\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    544\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    545\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Extract the trained Koopman matrix ---\n",
    "# (Make sure 'model' is your trained KoopmanUNet)\n",
    "model.eval() # Put model in evaluation mode\n",
    "K_matrix = model.koopman_operator.weight.detach().cpu().numpy()\n",
    "\n",
    "# --- 2. Perform Eigendecomposition ---\n",
    "eigenvalues, eigenvectors = np.linalg.eig(K_matrix)\n",
    "\n",
    "# Sort them by the magnitude (importance) of the eigenvalues\n",
    "sorted_indices = np.argsort(np.abs(eigenvalues))[::-1] # Sort descending\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "print(f\"Found {len(eigenvalues)} Koopman modes.\")\n",
    "\n",
    "# --- 3. Analyze the Eigenvalues (The \"Dynamics\") ---\n",
    "# Eigenvalues (λ) tell you the *temporal* behavior.\n",
    "# |\\lambda| ≈ 1.0 : \"Stable Modes\". Patterns that are preserved. (e.g., the digit's identity)\n",
    "# |\\lambda| < 1.0 : \"Decaying Modes\". Patterns that are removed. (e.g., noise, fuzzy edges)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.real(eigenvalues), np.imag(eigenvalues), 'o', markersize=5)\n",
    "plt.title(\"Eigenvalues in Complex Plane\")\n",
    "plt.xlabel(\"Real Part\")\n",
    "plt.ylabel(\"Imaginary Part\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.abs(eigenvalues), 'o-')\n",
    "plt.title(\"Eigenvalue Magnitudes\")\n",
    "plt.xlabel(\"Mode Index\")\n",
    "plt.ylabel(\"Magnitude |λ|\")\n",
    "plt.axhline(1.0, color='red', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# --- 4. Visualize the Eigenvectors (The \"Patterns\") ---\n",
    "# The eigenvectors (v) are the \"Koopman Modes\".\n",
    "# They live in the latent space (size 512). We need to pass them\n",
    "# through the DECODER to see what they look like as images.\n",
    "\n",
    "def visualize_mode(model, eigenvector_column):\n",
    "    # Convert eigenvector (a numpy column) to a tensor\n",
    "    mode_vector = torch.from_numpy(np.real(eigenvector_column)).float()\n",
    "    mode_vector = mode_vector.to(next(model.parameters()).device) # Move to GPU\n",
    "\n",
    "    # Reshape it to the bottleneck shape [batch, C, H, W]\n",
    "    mode_tensor = mode_vector.view(1, model.bottleneck_c, model.bottleneck_h, model.bottleneck_w)\n",
    "\n",
    "    # Create dummy skip connections (all zeros)\n",
    "    # This is a bit complex, but we need to match the shapes\n",
    "    dummy_t_emb = model.time_embedding(model.time_proj(torch.tensor([1.0], device=mode_tensor.device)))\n",
    "    dummy_skips = []\n",
    "\n",
    "    # We build skip connections by running a dummy input through the ENCODER\n",
    "    # (Using zeros as input is a common trick)\n",
    "    temp_x = torch.zeros(1, 1, config.image_size, config.image_size).to(mode_tensor.device)\n",
    "    for block in model.down_blocks:\n",
    "        temp_x, skips = block(temp_x, dummy_t_emb)\n",
    "        for skip in skips:\n",
    "            # We use zero skips to see the \"pure\" mode\n",
    "            dummy_skips.append(torch.zeros_like(skip))\n",
    "\n",
    "    # --- Run the DECODER ---\n",
    "    x = mode_tensor\n",
    "    for block in model.up_blocks:\n",
    "        skips_for_block = [dummy_skips.pop() for _ in range(block.resnets)]\n",
    "        x = block(x, skips_for_block, dummy_t_emb)\n",
    "\n",
    "    # Final output layers\n",
    "    x = model.conv_norm_out(x)\n",
    "    x = model.conv_act(x)\n",
    "    x = model.conv_out(x)\n",
    "\n",
    "    return x.detach().cpu()\n",
    "\n",
    "# --- Now, let's visualize the top 5 modes ---\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    mode_image = visualize_mode(model, eigenvectors[:, i])\n",
    "\n",
    "    # Denormalize image from [-1, 1] to [0, 1]\n",
    "    img = (mode_image[0].squeeze() / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"Mode {i}\\n|λ| = {np.abs(eigenvalues[i]):.3f}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "id": "623510f1d67bcbaa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
